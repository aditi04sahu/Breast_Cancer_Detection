{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4441334,"sourceType":"datasetVersion","datasetId":2600743},{"sourceId":10766393,"sourceType":"datasetVersion","datasetId":6678700},{"sourceId":10766548,"sourceType":"datasetVersion","datasetId":6678795},{"sourceId":260085,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":222344,"modelId":244103}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-04T09:04:55.235716Z","iopub.execute_input":"2025-03-04T09:04:55.235998Z","iopub.status.idle":"2025-03-04T09:05:14.093600Z","shell.execute_reply.started":"2025-03-04T09:04:55.235969Z","shell.execute_reply":"2025-03-04T09:05:14.092912Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PREPROCESSING","metadata":{}},{"cell_type":"code","source":"img_size=256\nBATCH_size=32\nchannels=3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T09:06:51.964883Z","iopub.execute_input":"2025-03-04T09:06:51.965209Z","iopub.status.idle":"2025-03-04T09:06:51.968947Z","shell.execute_reply.started":"2025-03-04T09:06:51.965183Z","shell.execute_reply":"2025-03-04T09:06:51.968155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nBASE_PATH = '/kaggle/input/ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/train'\nunique_classes = os.listdir(BASE_PATH)\nclass_index = [unique_classes[0], unique_classes[1]] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T09:06:54.622415Z","iopub.execute_input":"2025-03-04T09:06:54.622689Z","iopub.status.idle":"2025-03-04T09:06:54.929545Z","shell.execute_reply.started":"2025-03-04T09:06:54.622668Z","shell.execute_reply":"2025-03-04T09:06:54.928896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\ndataset = tf.keras.preprocessing.image_dataset_from_directory(\n    \"/kaggle/input/ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/train\", \n    shuffle=True,\n    image_size=(img_size, img_size),\n    batch_size=BATCH_size\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T09:06:58.441615Z","iopub.execute_input":"2025-03-04T09:06:58.441904Z","iopub.status.idle":"2025-03-04T09:07:14.831513Z","shell.execute_reply.started":"2025-03-04T09:06:58.441882Z","shell.execute_reply":"2025-03-04T09:07:14.830546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"/kaggle/input/ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/val\", \n    shuffle=True,\n    image_size=(img_size, img_size),\n    batch_size=BATCH_size\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:10.752823Z","iopub.execute_input":"2025-02-17T13:59:10.753138Z","iopub.status.idle":"2025-02-17T13:59:11.355899Z","shell.execute_reply.started":"2025-02-17T13:59:10.753114Z","shell.execute_reply":"2025-02-17T13:59:11.355046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names=dataset.class_names\nclass_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:15.037085Z","iopub.execute_input":"2025-02-17T13:59:15.037380Z","iopub.status.idle":"2025-02-17T13:59:15.043252Z","shell.execute_reply.started":"2025-02-17T13:59:15.037357Z","shell.execute_reply":"2025-02-17T13:59:15.042508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:18.115724Z","iopub.execute_input":"2025-02-17T13:59:18.116055Z","iopub.status.idle":"2025-02-17T13:59:18.121802Z","shell.execute_reply.started":"2025-02-17T13:59:18.116029Z","shell.execute_reply":"2025-02-17T13:59:18.121127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"254*32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:21.569381Z","iopub.execute_input":"2025-02-17T13:59:21.569667Z","iopub.status.idle":"2025-02-17T13:59:21.574552Z","shell.execute_reply.started":"2025-02-17T13:59:21.569645Z","shell.execute_reply":"2025-02-17T13:59:21.573760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfor img_batch,label_batch in dataset.take(1):\n    for i in range(12):\n        plt.subplot(3,4,i+1)\n        plt.imshow(img_batch[i].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")\n        plt.title(class_names[label_batch[i]])\n        #print(label_batch.numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:24.810426Z","iopub.execute_input":"2025-02-17T13:59:24.810755Z","iopub.status.idle":"2025-02-17T13:59:25.848957Z","shell.execute_reply.started":"2025-02-17T13:59:24.810725Z","shell.execute_reply":"2025-02-17T13:59:25.847957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_size=0.9\nlen(dataset)*train_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:30.901227Z","iopub.execute_input":"2025-02-17T13:59:30.901529Z","iopub.status.idle":"2025-02-17T13:59:30.907524Z","shell.execute_reply.started":"2025-02-17T13:59:30.901506Z","shell.execute_reply":"2025-02-17T13:59:30.906453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds= dataset.take(228)\nlen(train_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:34.257653Z","iopub.execute_input":"2025-02-17T13:59:34.258018Z","iopub.status.idle":"2025-02-17T13:59:34.265254Z","shell.execute_reply.started":"2025-02-17T13:59:34.257986Z","shell.execute_reply":"2025-02-17T13:59:34.264509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_ds=dataset.skip(228) \nlen(val_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:37.434234Z","iopub.execute_input":"2025-02-17T13:59:37.434521Z","iopub.status.idle":"2025-02-17T13:59:37.443672Z","shell.execute_reply.started":"2025-02-17T13:59:37.434499Z","shell.execute_reply":"2025-02-17T13:59:37.442843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dataset_partitions_tf(ds, train_split=0.9, val_split=0.1, shuffle=True, shuffle_size=10000):\n    ds_size = len(ds)\n\n    if shuffle:\n        ds = ds.shuffle(shuffle_size, seed=12)\n\n    # Calculate the number of training samples\n    train_size = int(train_split * ds_size)\n\n    # Calculate the number of validation samples\n    val_size = ds_size - train_size  # Allocate the rest to validation\n\n    # Create the training dataset\n    train_ds = ds.take(train_size)\n\n    # Create the validation dataset\n    val_ds = ds.skip(train_size).take(val_size)\n\n    return train_ds, val_ds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:41.541088Z","iopub.execute_input":"2025-02-17T13:59:41.541419Z","iopub.status.idle":"2025-02-17T13:59:41.546097Z","shell.execute_reply.started":"2025-02-17T13:59:41.541389Z","shell.execute_reply":"2025-02-17T13:59:41.545163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds,val_ds=get_dataset_partitions_tf(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:45.170819Z","iopub.execute_input":"2025-02-17T13:59:45.171161Z","iopub.status.idle":"2025-02-17T13:59:45.181929Z","shell.execute_reply.started":"2025-02-17T13:59:45.171130Z","shell.execute_reply":"2025-02-17T13:59:45.181147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:48.076015Z","iopub.execute_input":"2025-02-17T13:59:48.076309Z","iopub.status.idle":"2025-02-17T13:59:48.081812Z","shell.execute_reply.started":"2025-02-17T13:59:48.076287Z","shell.execute_reply":"2025-02-17T13:59:48.080978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(val_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:59:51.574645Z","iopub.execute_input":"2025-02-17T13:59:51.575003Z","iopub.status.idle":"2025-02-17T13:59:51.580396Z","shell.execute_reply.started":"2025-02-17T13:59:51.574971Z","shell.execute_reply":"2025-02-17T13:59:51.579556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds=train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\nval_ds=val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:00:30.674875Z","iopub.execute_input":"2025-02-17T14:00:30.675182Z","iopub.status.idle":"2025-02-17T14:00:30.686604Z","shell.execute_reply.started":"2025-02-17T14:00:30.675158Z","shell.execute_reply":"2025-02-17T14:00:30.685991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nresize_and_rescale = tf.keras.Sequential([\n    layers.Resizing(img_size, img_size),\n    layers.Rescaling(1.0 / 255)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:00:33.735020Z","iopub.execute_input":"2025-02-17T14:00:33.735309Z","iopub.status.idle":"2025-02-17T14:00:33.741116Z","shell.execute_reply.started":"2025-02-17T14:00:33.735286Z","shell.execute_reply":"2025-02-17T14:00:33.740177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resize_and_rescale = tf.keras.Sequential([\n    layers.Resizing(img_size, img_size),\n    layers.Rescaling(1.0 / 255)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:00:36.313588Z","iopub.execute_input":"2025-02-17T14:00:36.313936Z","iopub.status.idle":"2025-02-17T14:00:36.319878Z","shell.execute_reply.started":"2025-02-17T14:00:36.313906Z","shell.execute_reply":"2025-02-17T14:00:36.318893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_aug=tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2)\n    \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:00:39.199013Z","iopub.execute_input":"2025-02-17T14:00:39.199309Z","iopub.status.idle":"2025-02-17T14:00:39.208748Z","shell.execute_reply.started":"2025-02-17T14:00:39.199285Z","shell.execute_reply":"2025-02-17T14:00:39.208063Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN MODEL","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers, models\n\n# Define your input shape without the batch size\ninput_shape = (img_size, img_size, channels)\nn_classes = 2\n\n# Assuming resize_and_rescale and data_aug are preprocessing layers\nmodel = models.Sequential([\n    layers.InputLayer(shape=input_shape),  # Use `shape` instead of `input_shape`\n    resize_and_rescale,  # Preprocessing layer\n    data_aug,            # Data augmentation layer\n    layers.Conv2D(32, (3, 3), activation='relu', name=\"conv2d_1\"),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu', name=\"conv2d_2\"),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu', name=\"conv2d_3\"),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu', name=\"conv2d_4\"),  # Verify layer name\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu', name=\"conv2d_5\"),  # Last Conv layer\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(n_classes, activation='softmax'),\n])\n\n# No need to call model.build() since input shape is defined","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:00:43.119388Z","iopub.execute_input":"2025-02-17T14:00:43.119685Z","iopub.status.idle":"2025-02-17T14:00:44.122977Z","shell.execute_reply.started":"2025-02-17T14:00:43.119663Z","shell.execute_reply":"2025-02-17T14:00:44.122321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:00:48.092847Z","iopub.execute_input":"2025-02-17T14:00:48.093137Z","iopub.status.idle":"2025-02-17T14:00:48.098260Z","shell.execute_reply.started":"2025-02-17T14:00:48.093115Z","shell.execute_reply":"2025-02-17T14:00:48.097494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:00:59.418913Z","iopub.execute_input":"2025-02-17T14:00:59.419189Z","iopub.status.idle":"2025-02-17T14:00:59.441924Z","shell.execute_reply.started":"2025-02-17T14:00:59.419170Z","shell.execute_reply":"2025-02-17T14:00:59.441042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',           \n    patience=5,                   \n    restore_best_weights=True     \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:01:04.774549Z","iopub.execute_input":"2025-02-17T14:01:04.774908Z","iopub.status.idle":"2025-02-17T14:01:04.780964Z","shell.execute_reply.started":"2025-02-17T14:01:04.774878Z","shell.execute_reply":"2025-02-17T14:01:04.780126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n    metrics=['accuracy'] \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:01:09.623849Z","iopub.execute_input":"2025-02-17T14:01:09.624155Z","iopub.status.idle":"2025-02-17T14:01:09.636534Z","shell.execute_reply.started":"2025-02-17T14:01:09.624131Z","shell.execute_reply":"2025-02-17T14:01:09.635808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 30\nhistory = model.fit(\n    train_ds,\n    epochs=EPOCHS,\n    batch_size=BATCH_size,       \n    verbose=1,                   \n    validation_data=val_ds\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:01:13.125410Z","iopub.execute_input":"2025-02-17T14:01:13.125695Z","iopub.status.idle":"2025-02-17T14:06:14.862606Z","shell.execute_reply.started":"2025-02-17T14:01:13.125671Z","shell.execute_reply":"2025-02-17T14:06:14.861662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN ANALYSIS","metadata":{}},{"cell_type":"code","source":"scores=model.evaluate(test_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:42:31.562997Z","iopub.execute_input":"2025-02-17T06:42:31.563286Z","iopub.status.idle":"2025-02-17T06:42:33.775977Z","shell.execute_reply.started":"2025-02-17T06:42:31.563265Z","shell.execute_reply":"2025-02-17T06:42:33.775136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:42:44.446110Z","iopub.execute_input":"2025-02-17T06:42:44.446417Z","iopub.status.idle":"2025-02-17T06:42:44.451215Z","shell.execute_reply.started":"2025-02-17T06:42:44.446391Z","shell.execute_reply":"2025-02-17T06:42:44.450541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history.params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:42:55.588945Z","iopub.execute_input":"2025-02-17T06:42:55.589219Z","iopub.status.idle":"2025-02-17T06:42:55.594194Z","shell.execute_reply.started":"2025-02-17T06:42:55.589199Z","shell.execute_reply":"2025-02-17T06:42:55.593260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history.history.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:43:05.517479Z","iopub.execute_input":"2025-02-17T06:43:05.517808Z","iopub.status.idle":"2025-02-17T06:43:05.522919Z","shell.execute_reply.started":"2025-02-17T06:43:05.517786Z","shell.execute_reply":"2025-02-17T06:43:05.522077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CNN_train_acc=history.history['accuracy']\nprint(\"Training Accuracy:\", CNN_train_acc[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:43:13.556728Z","iopub.execute_input":"2025-02-17T06:43:13.557011Z","iopub.status.idle":"2025-02-17T06:43:13.561472Z","shell.execute_reply.started":"2025-02-17T06:43:13.556992Z","shell.execute_reply":"2025-02-17T06:43:13.560548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CNN_val_acc = history.history['val_accuracy']\nprint(\"Validation Accuracy:\", CNN_val_acc[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:43:23.558408Z","iopub.execute_input":"2025-02-17T06:43:23.558762Z","iopub.status.idle":"2025-02-17T06:43:23.563310Z","shell.execute_reply.started":"2025-02-17T06:43:23.558735Z","shell.execute_reply":"2025-02-17T06:43:23.562590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cnn_acc= history.history['accuracy']\ncnn_val_acc= history.history['val_accuracy']\ncnn_loss= history.history['loss']\ncnn_val_loss= history.history['val_loss']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:43:33.042279Z","iopub.execute_input":"2025-02-17T06:43:33.042588Z","iopub.status.idle":"2025-02-17T06:43:33.046360Z","shell.execute_reply.started":"2025-02-17T06:43:33.042565Z","shell.execute_reply":"2025-02-17T06:43:33.045665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the actual number of epochs run\nactual_epochs = len(history.history['loss'])\n\n# Plot Training and Validation Accuracy and Loss\nplt.figure(figsize=(14, 6))\n\n# Accuracy Plot\nplt.subplot(1, 2, 1)\nplt.plot(range(actual_epochs), history.history['accuracy'], label='Training Accuracy', marker='o', markevery=5, linewidth=1.5)\nplt.plot(range(actual_epochs), history.history['val_accuracy'], label='Validation Accuracy', marker='o', markevery=5, linewidth=1.5)\nplt.xlabel('Epochs', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('Training and Validation Accuracy', fontsize=14)\nplt.legend(loc='lower right', fontsize=10)\n\n# Loss Plot\nplt.subplot(1, 2, 2)\nplt.plot(range(actual_epochs), history.history['loss'], label='Training Loss', marker='o', markevery=5, linewidth=1.5)\nplt.plot(range(actual_epochs), history.history['val_loss'], label='Validation Loss', marker='o', markevery=5, linewidth=1.5)\nplt.xlabel('Epochs', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('Training and Validation Loss', fontsize=14)\nplt.legend(loc='upper right', fontsize=10)\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:43:42.917045Z","iopub.execute_input":"2025-02-17T06:43:42.917425Z","iopub.status.idle":"2025-02-17T06:43:43.573727Z","shell.execute_reply.started":"2025-02-17T06:43:42.917393Z","shell.execute_reply":"2025-02-17T06:43:43.572701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate accuracy on the validation set\nval_loss, val_accuracy = model.evaluate(val_ds)\nprint(f\"Validation Accuracy: {val_accuracy:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:44:10.864747Z","iopub.execute_input":"2025-02-17T06:44:10.865151Z","iopub.status.idle":"2025-02-17T06:44:11.045719Z","shell.execute_reply.started":"2025-02-17T06:44:10.865121Z","shell.execute_reply":"2025-02-17T06:44:11.044837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\n# Extract true labels and predictions from the validation set\ny_true = []\ny_pred = []\n\nfor images, labels in val_ds:\n    predictions = model.predict(images)\n    y_true.extend(labels.numpy())                   # True labels\n    y_pred.extend(np.argmax(predictions, axis=1))  # Predicted class indices\n\n# Generate classification report\nreport = classification_report(y_true, y_pred, target_names=class_names)\nprint(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:44:19.834599Z","iopub.execute_input":"2025-02-17T06:44:19.834922Z","iopub.status.idle":"2025-02-17T06:44:22.059094Z","shell.execute_reply.started":"2025-02-17T06:44:19.834897Z","shell.execute_reply":"2025-02-17T06:44:22.058232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate model and print validation metrics\nval_metrics = model.evaluate(val_ds, return_dict=True)\nprint(\"Validation Metrics:\")\nfor metric, value in val_metrics.items():\n    print(f\"{metric}: {value:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:44:36.500824Z","iopub.execute_input":"2025-02-17T06:44:36.501401Z","iopub.status.idle":"2025-02-17T06:44:36.676959Z","shell.execute_reply.started":"2025-02-17T06:44:36.501373Z","shell.execute_reply":"2025-02-17T06:44:36.676289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers\ny_true = []\ny_pred = []\n\nfor images, labels in test_ds:\n    preds = model.predict(images)  # Get predicted probabilities\n    y_true.extend(labels.numpy())   # True labels\n    y_pred.extend(preds[:, 1])      # Get the probabilities for the positive class\n\n# Convert lists to numpy arrays\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n\n# Calculate the ROC curve\nfpr, tpr, thresholds = roc_curve(y_true, y_pred)\nroc_auc = auc(fpr, tpr)\n\n# Plotting the ROC curve\nplt.figure(figsize=(8, 5))\nplt.plot(fpr, tpr, color='blue', label='ROC curve (area = {:.2f})'.format(roc_auc))\nplt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line (chance level)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.grid()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:44:46.654917Z","iopub.execute_input":"2025-02-17T06:44:46.655196Z","iopub.status.idle":"2025-02-17T06:44:49.033342Z","shell.execute_reply.started":"2025-02-17T06:44:46.655175Z","shell.execute_reply":"2025-02-17T06:44:49.032582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RESNET","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nresnet_model= Sequential()\n\npretrained_model=tf.keras.applications.ResNet50(include_top=False,\n                                               input_shape=(256,256,3),\n                                               pooling='avg',\n                                               classes=2,\n                                               weights='imagenet')\nfor layer in pretrained_model.layers: \n    layer.trainable = False\nresnet_model.add(pretrained_model)\nresnet_model.add(Flatten())\nresnet_model.add(Dense(512,activation='relu'))\nresnet_model.add(Dense(2,activation='softmax'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:45:05.848295Z","iopub.execute_input":"2025-02-17T06:45:05.848609Z","iopub.status.idle":"2025-02-17T06:45:07.473082Z","shell.execute_reply.started":"2025-02-17T06:45:05.848583Z","shell.execute_reply":"2025-02-17T06:45:07.472372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:45:21.636110Z","iopub.execute_input":"2025-02-17T06:45:21.636394Z","iopub.status.idle":"2025-02-17T06:45:21.656448Z","shell.execute_reply.started":"2025-02-17T06:45:21.636373Z","shell.execute_reply":"2025-02-17T06:45:21.655730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',           \n    patience=5,                   \n    restore_best_weights=True     \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:45:30.160891Z","iopub.execute_input":"2025-02-17T06:45:30.161170Z","iopub.status.idle":"2025-02-17T06:45:30.165200Z","shell.execute_reply.started":"2025-02-17T06:45:30.161150Z","shell.execute_reply":"2025-02-17T06:45:30.164296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\nresnet_model.compile(optimizer=Adam(),\n                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n                     metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:45:38.330227Z","iopub.execute_input":"2025-02-17T06:45:38.330572Z","iopub.status.idle":"2025-02-17T06:45:38.341689Z","shell.execute_reply.started":"2025-02-17T06:45:38.330541Z","shell.execute_reply":"2025-02-17T06:45:38.340972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS=50\nhistory_resnet = resnet_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    callbacks=[early_stopping] \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:45:46.899736Z","iopub.execute_input":"2025-02-17T06:45:46.900016Z","iopub.status.idle":"2025-02-17T06:50:40.432807Z","shell.execute_reply.started":"2025-02-17T06:45:46.899996Z","shell.execute_reply":"2025-02-17T06:50:40.432103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RESNET ANALYSIS","metadata":{}},{"cell_type":"code","source":"scores_resnet=resnet_model.evaluate(test_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:51:16.610574Z","iopub.execute_input":"2025-02-17T06:51:16.610876Z","iopub.status.idle":"2025-02-17T06:51:20.839577Z","shell.execute_reply.started":"2025-02-17T06:51:16.610853Z","shell.execute_reply":"2025-02-17T06:51:20.838751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores_resnet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:51:27.929920Z","iopub.execute_input":"2025-02-17T06:51:27.930229Z","iopub.status.idle":"2025-02-17T06:51:27.935156Z","shell.execute_reply.started":"2025-02-17T06:51:27.930201Z","shell.execute_reply":"2025-02-17T06:51:27.934243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_resnet.params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:51:39.532928Z","iopub.execute_input":"2025-02-17T06:51:39.533215Z","iopub.status.idle":"2025-02-17T06:51:39.538108Z","shell.execute_reply.started":"2025-02-17T06:51:39.533194Z","shell.execute_reply":"2025-02-17T06:51:39.537411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_resnet.history['accuracy']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:51:48.416212Z","iopub.execute_input":"2025-02-17T06:51:48.416527Z","iopub.status.idle":"2025-02-17T06:51:48.421398Z","shell.execute_reply.started":"2025-02-17T06:51:48.416475Z","shell.execute_reply":"2025-02-17T06:51:48.420770Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Resnet_train_acc=history_resnet.history['accuracy']\nprint(\"Training Accuracy:\", Resnet_train_acc[-1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:51:57.875634Z","iopub.execute_input":"2025-02-17T06:51:57.875952Z","iopub.status.idle":"2025-02-17T06:51:57.881087Z","shell.execute_reply.started":"2025-02-17T06:51:57.875927Z","shell.execute_reply":"2025-02-17T06:51:57.880226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Resnet_val_acc= history_resnet.history['val_accuracy']\nprint(\"Validation Accuracy:\", Resnet_val_acc[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:52:07.394384Z","iopub.execute_input":"2025-02-17T06:52:07.394737Z","iopub.status.idle":"2025-02-17T06:52:07.399366Z","shell.execute_reply.started":"2025-02-17T06:52:07.394710Z","shell.execute_reply":"2025-02-17T06:52:07.398577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet_acc= history_resnet.history['accuracy']\nresnet_val_acc= history_resnet.history['val_accuracy']\nresnet_loss= history_resnet.history['loss']\nresnet_val_loss= history_resnet.history['val_loss']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:52:16.639743Z","iopub.execute_input":"2025-02-17T06:52:16.640066Z","iopub.status.idle":"2025-02-17T06:52:16.643970Z","shell.execute_reply.started":"2025-02-17T06:52:16.640040Z","shell.execute_reply":"2025-02-17T06:52:16.643159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the number of epochs actually run (it will be less than or equal to EPOCHS)\nepochs_run = len(history_resnet.history['accuracy'])\n\n# Plot Training and Validation Accuracy\nplt.figure(figsize=(9, 4))\nplt.subplot(1, 2, 1)\nplt.plot(range(epochs_run), history_resnet.history['accuracy'], label='Training Accuracy',marker='o')\nplt.plot(range(epochs_run), history_resnet.history['val_accuracy'], label='Validation Accuracy',marker='o')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\n# Plot Training and Validation Loss\n\nplt.subplot(1, 2, 2)\nplt.plot(range(epochs_run), history_resnet.history['loss'], label='Training Loss',marker='o')\nplt.plot(range(epochs_run), history_resnet.history['val_loss'], label='Validation Loss',marker='o')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Loss')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:52:30.099164Z","iopub.execute_input":"2025-02-17T06:52:30.099474Z","iopub.status.idle":"2025-02-17T06:52:30.518067Z","shell.execute_reply.started":"2025-02-17T06:52:30.099450Z","shell.execute_reply":"2025-02-17T06:52:30.517168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate accuracy on the validation set\nval_loss, val_accuracy = resnet_model.evaluate(val_ds)\nprint(f\"Validation Accuracy: {val_accuracy:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:52:40.669150Z","iopub.execute_input":"2025-02-17T06:52:40.669457Z","iopub.status.idle":"2025-02-17T06:52:42.004254Z","shell.execute_reply.started":"2025-02-17T06:52:40.669429Z","shell.execute_reply":"2025-02-17T06:52:42.003392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\n# Extract true labels and predictions from the validation set\ny_true = []\ny_pred = []\n\nfor images, labels in val_ds:\n    predictions = resnet_model.predict(images)\n    y_true.extend(labels.numpy())                   # True labels\n    y_pred.extend(np.argmax(predictions, axis=1))  # Predicted class indices\n\n# Generate classification report\nreport = classification_report(y_true, y_pred, target_names=class_names)\nprint(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:52:49.502263Z","iopub.execute_input":"2025-02-17T06:52:49.502601Z","iopub.status.idle":"2025-02-17T06:52:55.192361Z","shell.execute_reply.started":"2025-02-17T06:52:49.502571Z","shell.execute_reply":"2025-02-17T06:52:55.191674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers\ny_true = []\ny_pred = []\n\nfor images, labels in test_ds:\n    preds = resnet_model.predict(images)  # Get predicted probabilities\n    y_true.extend(labels.numpy())   # True labels\n    y_pred.extend(preds[:, 1])      # Get the probabilities for the positive class\n\n# Convert lists to numpy arrays\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n\n# Calculate the ROC curve\nfpr, tpr, thresholds = roc_curve(y_true, y_pred)\nroc_auc = auc(fpr, tpr)\n\n# Plotting the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', label='ROC curve (area = {:.2f})'.format(roc_auc))\nplt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line (chance level)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.grid()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:53:04.251996Z","iopub.execute_input":"2025-02-17T06:53:04.252283Z","iopub.status.idle":"2025-02-17T06:53:10.935964Z","shell.execute_reply.started":"2025-02-17T06:53:04.252261Z","shell.execute_reply":"2025-02-17T06:53:10.935106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate model and print validation metrics\nval_metrics = resnet_model.evaluate(val_ds, return_dict=True)\nprint(\"Validation Metrics:\")\nfor metric, value in val_metrics.items():\n    print(f\"{metric}: {value:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:53:16.867251Z","iopub.execute_input":"2025-02-17T06:53:16.867623Z","iopub.status.idle":"2025-02-17T06:53:18.225813Z","shell.execute_reply.started":"2025-02-17T06:53:16.867593Z","shell.execute_reply":"2025-02-17T06:53:18.225100Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EFFICIENT NET","metadata":{}},{"cell_type":"code","source":"!pip install efficientnet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:53:29.748448Z","iopub.execute_input":"2025-02-17T06:53:29.748811Z","iopub.status.idle":"2025-02-17T06:53:34.380673Z","shell.execute_reply.started":"2025-02-17T06:53:29.748776Z","shell.execute_reply":"2025-02-17T06:53:34.379558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB0\nimport efficientnet.tfkeras as efn\n\nbase_model = efn.EfficientNetB0(input_shape = (256, 256, 3), include_top = False, weights = 'imagenet')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:53:43.276063Z","iopub.execute_input":"2025-02-17T06:53:43.276390Z","iopub.status.idle":"2025-02-17T06:53:45.357063Z","shell.execute_reply.started":"2025-02-17T06:53:43.276363Z","shell.execute_reply":"2025-02-17T06:53:45.356130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for layer in base_model.layers:\n    layer.trainable = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:53:54.776807Z","iopub.execute_input":"2025-02-17T06:53:54.777971Z","iopub.status.idle":"2025-02-17T06:53:54.785242Z","shell.execute_reply.started":"2025-02-17T06:53:54.777936Z","shell.execute_reply":"2025-02-17T06:53:54.784510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Flatten, Dense, Dropout\nfrom tensorflow.keras.models import Model\nx = base_model.output\nx = Flatten()(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\n\n# Add a final sigmoid layer with 1 node for classification output\npredictions = Dense(1, activation=\"sigmoid\")(x)\nmodel_efficientNet = Model(inputs=base_model.input, outputs=predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:54:08.998345Z","iopub.execute_input":"2025-02-17T06:54:08.998686Z","iopub.status.idle":"2025-02-17T06:54:09.037235Z","shell.execute_reply.started":"2025-02-17T06:54:08.998660Z","shell.execute_reply":"2025-02-17T06:54:09.036464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop\nmodel_efficientNet.compile(optimizer=RMSprop(learning_rate=0.0001, decay=1e-6), \n                           loss='binary_crossentropy', \n                           metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:54:23.626700Z","iopub.execute_input":"2025-02-17T06:54:23.627037Z","iopub.status.idle":"2025-02-17T06:54:23.640307Z","shell.execute_reply.started":"2025-02-17T06:54:23.627013Z","shell.execute_reply":"2025-02-17T06:54:23.639552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(\n    monitor='val_loss',          \n    patience=5,                  \n    restore_best_weights=True     \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:54:35.226691Z","iopub.execute_input":"2025-02-17T06:54:35.226991Z","iopub.status.idle":"2025-02-17T06:54:35.231576Z","shell.execute_reply.started":"2025-02-17T06:54:35.226971Z","shell.execute_reply":"2025-02-17T06:54:35.230821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_efficientNet = model_efficientNet.fit(\n    train_ds,\n    validation_data=val_ds,\n    steps_per_epoch=100,\n    epochs=EPOCHS,\n    callbacks=[early_stopping] \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:54:45.340154Z","iopub.execute_input":"2025-02-17T06:54:45.340497Z","iopub.status.idle":"2025-02-17T06:57:02.270455Z","shell.execute_reply.started":"2025-02-17T06:54:45.340471Z","shell.execute_reply":"2025-02-17T06:57:02.269575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ANALYSIS OF EFFICIENT NET","metadata":{}},{"cell_type":"code","source":"scores_efficientNet=model_efficientNet.evaluate(test_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:57:14.343418Z","iopub.execute_input":"2025-02-17T06:57:14.343758Z","iopub.status.idle":"2025-02-17T06:57:19.621160Z","shell.execute_reply.started":"2025-02-17T06:57:14.343733Z","shell.execute_reply":"2025-02-17T06:57:19.620072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores_efficientNet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:57:23.076987Z","iopub.execute_input":"2025-02-17T06:57:23.077312Z","iopub.status.idle":"2025-02-17T06:57:23.082090Z","shell.execute_reply.started":"2025-02-17T06:57:23.077284Z","shell.execute_reply":"2025-02-17T06:57:23.081386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_efficientNet.params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:57:31.940179Z","iopub.execute_input":"2025-02-17T06:57:31.940457Z","iopub.status.idle":"2025-02-17T06:57:31.945703Z","shell.execute_reply.started":"2025-02-17T06:57:31.940437Z","shell.execute_reply":"2025-02-17T06:57:31.944783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_efficientNet.history['accuracy']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:57:41.853135Z","iopub.execute_input":"2025-02-17T06:57:41.853555Z","iopub.status.idle":"2025-02-17T06:57:41.858836Z","shell.execute_reply.started":"2025-02-17T06:57:41.853521Z","shell.execute_reply":"2025-02-17T06:57:41.858012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Eff_train_acc = history_efficientNet.history['accuracy']\nprint(\"Training Accuracy:\", Eff_train_acc[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:57:50.678173Z","iopub.execute_input":"2025-02-17T06:57:50.678468Z","iopub.status.idle":"2025-02-17T06:57:50.683359Z","shell.execute_reply.started":"2025-02-17T06:57:50.678447Z","shell.execute_reply":"2025-02-17T06:57:50.682379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Eff_val_acc = history_efficientNet.history['val_accuracy']\nprint(\"Validation Accuracy:\", Eff_val_acc[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:58:03.577855Z","iopub.execute_input":"2025-02-17T06:58:03.578167Z","iopub.status.idle":"2025-02-17T06:58:03.583185Z","shell.execute_reply.started":"2025-02-17T06:58:03.578142Z","shell.execute_reply":"2025-02-17T06:58:03.582333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"efficientNet_acc= history_efficientNet.history['accuracy']\nefficientNet_val_acc= history_efficientNet.history['val_accuracy']\nefficientNet_loss= history_efficientNet.history['loss']\nefficientNet_val_loss= history_efficientNet.history['val_loss']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:58:12.192015Z","iopub.execute_input":"2025-02-17T06:58:12.192320Z","iopub.status.idle":"2025-02-17T06:58:12.196171Z","shell.execute_reply.started":"2025-02-17T06:58:12.192295Z","shell.execute_reply":"2025-02-17T06:58:12.195241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Determine the number of epochs actually run\nepochs_run = len(efficientNet_acc)\n\n# Plot Training and Validation Accuracy\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(range(epochs_run), efficientNet_acc, label='Training Accuracy', marker='o')\nplt.plot(range(epochs_run), efficientNet_val_acc, label='Validation Accuracy', marker='o')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\n# Plot Training and Validation Loss\nplt.subplot(1, 2, 2)\nplt.plot(range(epochs_run), efficientNet_loss, label='Training Loss', marker='o')\nplt.plot(range(epochs_run), efficientNet_val_loss, label='Validation Loss', marker='o')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\n\n# Adjust layout and show the plots\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:58:20.751643Z","iopub.execute_input":"2025-02-17T06:58:20.751949Z","iopub.status.idle":"2025-02-17T06:58:21.155546Z","shell.execute_reply.started":"2025-02-17T06:58:20.751926Z","shell.execute_reply":"2025-02-17T06:58:21.154627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate accuracy on the validation set\nval_loss, val_accuracy = model_efficientNet.evaluate(val_ds)\nprint(f\"Validation Accuracy: {val_accuracy:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:58:32.248306Z","iopub.execute_input":"2025-02-17T06:58:32.248652Z","iopub.status.idle":"2025-02-17T06:58:33.073956Z","shell.execute_reply.started":"2025-02-17T06:58:32.248624Z","shell.execute_reply":"2025-02-17T06:58:33.073227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\n# Extract true labels and predictions from the validation set\ny_true = []\ny_pred = []\n\nfor images, labels in val_ds:\n    predictions = model_efficientNet.predict(images)\n    y_true.extend(labels.numpy())                   # True labels\n    y_pred.extend(np.argmax(predictions, axis=1))  # Predicted class indices\n\n# Generate classification report\nreport = classification_report(y_true, y_pred, target_names=class_names)\nprint(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:58:48.592812Z","iopub.execute_input":"2025-02-17T06:58:48.593141Z","iopub.status.idle":"2025-02-17T06:58:55.554251Z","shell.execute_reply.started":"2025-02-17T06:58:48.593116Z","shell.execute_reply":"2025-02-17T06:58:55.553402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc_curve(model, test_ds):\n    y_true = []\n    y_pred = []\n    \n    # Get predictions and true labels\n    for images, labels in test_ds:\n        preds = model_efficientNet.predict(images)         # Model predictions\n        y_true.extend(labels.numpy())         # True labels\n        y_pred.extend(preds.flatten())        # Flatten predictions for positive class\n    \n    # Convert lists to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate ROC curve\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    roc_auc = auc(fpr, tpr)\n    \n    # Plot the ROC curve\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line (chance level)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc='lower right')\n    plt.grid()\n    plt.show()\n\n# Usage Example:\n# Assuming `model` is your trained model and `test_ds` is your test dataset\nplot_roc_curve(model, test_ds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:59:03.443689Z","iopub.execute_input":"2025-02-17T06:59:03.443982Z","iopub.status.idle":"2025-02-17T06:59:10.954839Z","shell.execute_reply.started":"2025-02-17T06:59:03.443960Z","shell.execute_reply":"2025-02-17T06:59:10.953904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate model and print validation metrics\nval_metrics = model_efficientNet.evaluate(val_ds, return_dict=True)\nprint(\"Validation Metrics:\")\nfor metric, value in val_metrics.items():\n    print(f\"{metric}: {value:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:59:22.958516Z","iopub.execute_input":"2025-02-17T06:59:22.958822Z","iopub.status.idle":"2025-02-17T06:59:23.785232Z","shell.execute_reply.started":"2025-02-17T06:59:22.958790Z","shell.execute_reply":"2025-02-17T06:59:23.784592Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# COMPARATIVE STUDY","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Evaluate models and extract accuracy\nscores = model.evaluate(test_ds)\nscores_resnet = resnet_model.evaluate(test_ds)\nscores_efficientNet = model_efficientNet.evaluate(test_ds)\n\n# Assuming the accuracy is the second value in the `scores` array\naccuracy_model = scores[1]\naccuracy_resnet = scores_resnet[1]\naccuracy_efficientNet = scores_efficientNet[1]\n\n# Create a list of model names and corresponding accuracies\nmodel_names = ['Base Model', 'ResNet', 'EfficientNet']\naccuracies = [accuracy_model, accuracy_resnet, accuracy_efficientNet]\n\n# Plotting the accuracies on a bar graph\nplt.figure(figsize=(6, 4))  # Decreased figure size\nplt.bar(model_names, accuracies, color=['blue', 'green', 'orange'], width=0.4)  # Reduced bar width\n\n# Adding titles and labels\nplt.title('Test Accuracy Comparison of Models', fontsize=12)\nplt.xlabel('Models', fontsize=10)\nplt.ylabel('Test Accuracy', fontsize=10)\n\n# Show the plot\nplt.tight_layout()  # Adjust layout to prevent overlap\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:59:35.444745Z","iopub.execute_input":"2025-02-17T06:59:35.445031Z","iopub.status.idle":"2025-02-17T06:59:39.406943Z","shell.execute_reply.started":"2025-02-17T06:59:35.445010Z","shell.execute_reply":"2025-02-17T06:59:39.406232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Example accuracy and validation accuracy for the models\n# Replace these with your actual values\nmodels = ['CNN', 'ResNet', 'EfficientNet']\n# Assuming CNN_train_acc, Resnet_train_acc, Eff_train_acc are lists with accuracy per epoch\n# Use the last value (or final accuracy) for each model\naccuracy = [CNN_train_acc[-1], Resnet_train_acc[-1], Eff_train_acc[-1]]  # Final Training accuracy for CNN, ResNet, and EfficientNet\nval_accuracy = [CNN_val_acc[-1], Resnet_val_acc[-1], Eff_val_acc[-1]]  # Final Validation accuracy for CNN, ResNet, and EfficientNet\n\n# Set Seaborn style and customize the background\nsns.set_style(\"whitegrid\")\nplt.rcParams['axes.facecolor'] = '#f7f7f7'  # Off-white background color\n\n# Setting up the bar positions\nx = np.arange(len(models))  # the label locations\nwidth = 0.2  # Smaller bar width\n\n# Create the bar chart\nfig, ax = plt.subplots(figsize=(6, 4))  # Smaller figure size\nbars1 = ax.bar(x - width/2, accuracy, width, label='Training Accuracy', color='black')\nbars2 = ax.bar(x + width/2, val_accuracy, width, label='Validation Accuracy', color='red')\n\n# Add labels, title, and legend\nax.set_xlabel('Models', fontsize=12)\nax.set_ylabel('Accuracy', fontsize=12)\nax.set_title('Comparative Study of Model Accuracy', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(models, fontsize=10)\nax.legend(fontsize=10, loc='lower right')\n\n# Annotate bars with values\nfor bar in bars1:\n    height = bar.get_height()\n    ax.annotate(f'{height:.2f}', \n                xy=(bar.get_x() + bar.get_width() / 2, height), \n                xytext=(0, 3),  # Offset by 3 points\n                textcoords=\"offset points\",\n                ha='center', va='bottom', fontsize=8)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.annotate(f'{height:.2f}', \n                xy=(bar.get_x() + bar.get_width() / 2, height), \n                xytext=(0, 3), \n                textcoords=\"offset points\",\n                ha='center', va='bottom', fontsize=8)\n\n# Adjust layout and show plot\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T06:59:52.859014Z","iopub.execute_input":"2025-02-17T06:59:52.859326Z","iopub.status.idle":"2025-02-17T06:59:53.375663Z","shell.execute_reply.started":"2025-02-17T06:59:52.859301Z","shell.execute_reply":"2025-02-17T06:59:53.374698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Correct model path\nmodel_path = \"/kaggle/input/my_model/keras/default/2/my_model.h5\"\n\n# Load the model\nmodel = load_model(model_path)\n\nprint(\"Model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:17:37.273426Z","iopub.execute_input":"2025-02-17T14:17:37.273735Z","iopub.status.idle":"2025-02-17T14:17:41.102217Z","shell.execute_reply.started":"2025-02-17T14:17:37.273711Z","shell.execute_reply":"2025-02-17T14:17:41.101427Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Example from dataset","metadata":{}},{"cell_type":"markdown","source":"CLASSIFICATION CODE","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport cv2\n\n# Assuming your model is already trained and compiled\n# If not, you can load a saved model using tf.keras.models.load_model('your_model_path')\n\n# Function to preprocess the image\ndef get_img_array(img_path, size):\n    # Load the image with the target size\n    img = image.load_img(img_path, target_size=size)\n    # Convert the image to a numpy array\n    array = image.img_to_array(img)\n    # Expand dimensions to create a batch of size 1\n    array = np.expand_dims(array, axis=0)\n    return array\n\n# Path to the image you want to visualize\nimg_path = '/kaggle/input/ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/val/malignant/malignant (1)-rotated1.png'  # Replace with your image path\n\n# Prepare the image\nimg_array = get_img_array(img_path, size=(img_size, img_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:07:24.706523Z","iopub.execute_input":"2025-02-17T14:07:24.706865Z","iopub.status.idle":"2025-02-17T14:07:24.718158Z","shell.execute_reply.started":"2025-02-17T14:07:24.706832Z","shell.execute_reply":"2025-02-17T14:07:24.717125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict the class of the image\npredictions = model.predict(img_array)\npredicted_class = np.argmax(predictions[0])\nclass_names = ['benign', 'malignant']  # Ensure these match your dataset's class names\npredicted_label = class_names[predicted_class]\n\nprint(f\"The model predicts this image is: {predicted_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:07:29.981345Z","iopub.execute_input":"2025-02-17T14:07:29.981679Z","iopub.status.idle":"2025-02-17T14:07:30.047637Z","shell.execute_reply.started":"2025-02-17T14:07:29.981647Z","shell.execute_reply":"2025-02-17T14:07:30.046971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PERCENTAGE AND HEATMAP CODE","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import load_model\n\n# Load the trained model\nmodel_path = \"/kaggle/input/my_model/keras/default/2/my_model.h5\"\nmodel = load_model(model_path)\n\n# Function to preprocess input image\ndef preprocess_image(img_path, img_size=224):\n    img = image.load_img(img_path, target_size=(img_size, img_size))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = img_array / 255.0\n    return img_array, img\n\n# Function to generate Grad-CAM heatmap\ndef get_grad_cam(model, img_array, layer_name):\n    grad_model = tf.keras.models.Model(\n        inputs=model.input, \n        outputs=[model.get_layer(layer_name).output, model.output]\n    )\n    \n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        class_idx = np.argmax(predictions[0])\n        loss = predictions[:, class_idx]\n    \n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0] * pooled_grads\n    heatmap = np.mean(conv_outputs, axis=-1)\n    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n    return heatmap\n\n# Function to overlay heatmap on image\ndef overlay_heatmap(img, heatmap, alpha=0.4):\n    heatmap = cv2.resize(heatmap, (img.size[0], img.size[1]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    img = np.array(img)\n    overlay = cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0)\n    return overlay\n\n# Function to calculate affected percentage\ndef calculate_affected_percentage(heatmap, threshold=0.5):\n    affected_area = np.sum(heatmap > threshold)\n    total_area = heatmap.size\n    affected_percentage = (affected_area / total_area) * 100\n    return affected_percentage\n\n# Load and process image\nimg_path = \"/kaggle/input/ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/val/malignant/malignant (1)-rotated1.png\"\nimg_array, img = preprocess_image(img_path, img_size=224)\n\n# Generate Grad-CAM heatmap\nlayer_name = \"block5_conv3\"  # Adjust based on your model architecture\nheatmap = get_grad_cam(model, img_array, layer_name)\n\n# Calculate affected percentage\naffected_percentage = calculate_affected_percentage(heatmap)\n\n# Overlay heatmap\noverlayed_img = overlay_heatmap(img, heatmap)\n\n# Display results\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\naxes[0].imshow(img)\naxes[0].set_title(\"Original Image\")\naxes[1].imshow(overlayed_img)\naxes[1].set_title(f\"Grad-CAM Heatmap\\nAffected Area: {affected_percentage:.2f}%\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:17:56.924967Z","iopub.execute_input":"2025-02-17T14:17:56.925377Z","iopub.status.idle":"2025-02-17T14:18:02.176265Z","shell.execute_reply.started":"2025-02-17T14:17:56.925344Z","shell.execute_reply":"2025-02-17T14:18:02.175338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Example from outside image","metadata":{}},{"cell_type":"markdown","source":"CLASSIFICATION CODE","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport cv2\n\n# Assuming your model is already trained and compiled\n# If not, you can load a saved model using tf.keras.models.load_model('your_model_path')\n\n# Function to preprocess the image\ndef get_img_array(img_path, size):\n    # Load the image with the target size\n    img = image.load_img(img_path, target_size=size)\n    # Convert the image to a numpy array\n    array = image.img_to_array(img)\n    # Expand dimensions to create a batch of size 1\n    array = np.expand_dims(array, axis=0)\n    return array\n\n# Path to the image you want to visualize\nimg_path = '/kaggle/input/cancer2/Screenshot 2025-02-16 215943.png'  # Replace with your image path\n\n# Prepare the image\nimg_array = get_img_array(img_path, size=(img_size, img_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:08:30.532643Z","iopub.execute_input":"2025-02-17T14:08:30.533010Z","iopub.status.idle":"2025-02-17T14:08:30.547312Z","shell.execute_reply.started":"2025-02-17T14:08:30.532981Z","shell.execute_reply":"2025-02-17T14:08:30.546352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict the class of the image\npredictions = model.predict(img_array)\npredicted_class = np.argmax(predictions[0])\nclass_names = ['benign', 'malignant']  # Ensure these match your dataset's class names\npredicted_label = class_names[predicted_class]\n\nprint(f\"The model predicts this image is: {predicted_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:08:52.531611Z","iopub.execute_input":"2025-02-17T14:08:52.531965Z","iopub.status.idle":"2025-02-17T14:08:52.594112Z","shell.execute_reply.started":"2025-02-17T14:08:52.531933Z","shell.execute_reply":"2025-02-17T14:08:52.593388Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PERCENTAGE AND HEATMAP CODE","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import load_model\n\n# Load the trained model\nmodel_path = \"/kaggle/input/my_model/keras/default/2/my_model.h5\"\nmodel = load_model(model_path)\n\n# Function to preprocess input image\ndef preprocess_image(img_path, img_size=224):\n    img = image.load_img(img_path, target_size=(img_size, img_size))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = img_array / 255.0\n    return img_array, img\n\n# Function to generate Grad-CAM heatmap\ndef get_grad_cam(model, img_array, layer_name):\n    grad_model = tf.keras.models.Model(\n        inputs=model.input, \n        outputs=[model.get_layer(layer_name).output, model.output]\n    )\n    \n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        class_idx = np.argmax(predictions[0])\n        loss = predictions[:, class_idx]\n    \n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0] * pooled_grads\n    heatmap = np.mean(conv_outputs, axis=-1)\n    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n    return heatmap\n\n# Function to overlay heatmap on image\ndef overlay_heatmap(img, heatmap, alpha=0.4):\n    heatmap = cv2.resize(heatmap, (img.size[0], img.size[1]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    img = np.array(img)\n    overlay = cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0)\n    return overlay\n\n# Function to calculate affected percentage\ndef calculate_affected_percentage(heatmap, threshold=0.5):\n    affected_area = np.sum(heatmap > threshold)\n    total_area = heatmap.size\n    affected_percentage = (affected_area / total_area) * 100\n    return affected_percentage\n\n# Load and process image\nimg_path = \"/kaggle/input/cancer2/Screenshot 2025-02-16 215943.png\"\nimg_array, img = preprocess_image(img_path, img_size=224)\n\n# Generate Grad-CAM heatmap\nlayer_name = \"block5_conv3\"  # Adjust based on your model architecture\nheatmap = get_grad_cam(model, img_array, layer_name)\n\n# Calculate affected percentage\naffected_percentage = calculate_affected_percentage(heatmap)\n\n# Overlay heatmap\noverlayed_img = overlay_heatmap(img, heatmap)\n\n# Display results\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\naxes[0].imshow(img)\naxes[0].set_title(\"Original Image\")\naxes[1].imshow(overlayed_img)\naxes[1].set_title(f\"Grad-CAM Heatmap\\nAffected Area: {affected_percentage:.2f}%\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:19:15.032103Z","iopub.execute_input":"2025-02-17T14:19:15.032415Z","iopub.status.idle":"2025-02-17T14:19:19.592498Z","shell.execute_reply.started":"2025-02-17T14:19:15.032391Z","shell.execute_reply":"2025-02-17T14:19:19.591587Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Example from outside image with benign","metadata":{}},{"cell_type":"markdown","source":"CLASSIFICATION CODE","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport cv2\n\n# Assuming your model is already trained and compiled\n# If not, you can load a saved model using tf.keras.models.load_model('your_model_path')\n\n# Function to preprocess the image\ndef get_img_array(img_path, size):\n    # Load the image with the target size\n    img = image.load_img(img_path, target_size=size)\n    # Convert the image to a numpy array\n    array = image.img_to_array(img)\n    # Expand dimensions to create a batch of size 1\n    array = np.expand_dims(array, axis=0)\n    return array\n\n# Path to the image you want to visualize\nimg_path = '/kaggle/input/cancer4/Screenshot 2025-02-16 214605.png'  # Replace with your image path\n\n# Prepare the image\nimg_array = get_img_array(img_path, size=(img_size, img_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T09:12:23.820714Z","iopub.execute_input":"2025-03-04T09:12:23.820919Z","iopub.status.idle":"2025-03-04T09:12:36.458543Z","shell.execute_reply.started":"2025-03-04T09:12:23.820899Z","shell.execute_reply":"2025-03-04T09:12:36.457327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict the class of the image\npredictions = model.predict(img_array)\npredicted_class = np.argmax(predictions[0])\nclass_names = ['benign', 'malignant']  # Ensure these match your dataset's class names\npredicted_label = class_names[predicted_class]\n\nprint(f\"The model predicts this image is: {predicted_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:10:32.036728Z","iopub.execute_input":"2025-02-17T14:10:32.037078Z","iopub.status.idle":"2025-02-17T14:10:32.098612Z","shell.execute_reply.started":"2025-02-17T14:10:32.037052Z","shell.execute_reply":"2025-02-17T14:10:32.097973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}